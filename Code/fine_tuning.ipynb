{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bf7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:22.570005Z",
     "iopub.status.busy": "2025-03-18T16:46:22.569754Z",
     "iopub.status.idle": "2025-03-18T16:46:30.190219Z",
     "shell.execute_reply": "2025-03-18T16:46:30.189523Z"
    },
    "papermill": {
     "duration": 7.624864,
     "end_time": "2025-03-18T16:46:30.191718",
     "exception": false,
     "start_time": "2025-03-18T16:46:22.566854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.path.abspath(os.sep)\n",
    "train_dataset_folder = \"../Dataset/fasterrcnn/train\"\n",
    "valid_dataset_folder = \"../Dataset/fasterrcnn/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae16b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:30.196898Z",
     "iopub.status.busy": "2025-03-18T16:46:30.196540Z",
     "iopub.status.idle": "2025-03-18T16:46:30.203982Z",
     "shell.execute_reply": "2025-03-18T16:46:30.203335Z"
    },
    "papermill": {
     "duration": 0.011078,
     "end_time": "2025-03-18T16:46:30.205176",
     "exception": false,
     "start_time": "2025-03-18T16:46:30.194098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "class GunDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None, train=True):\n",
    "      with open(annotation_dir, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "      self.image_dir = image_dir\n",
    "      self.transform = transform\n",
    "      self.train = train\n",
    "      self.images = coco_data['images']\n",
    "      self.annotations = coco_data['annotations']\n",
    "      self.classes = {\"gun\": 0}\n",
    "      \n",
    "      # Crea il mapping immagine -> annotazioni\n",
    "      self.img_to_anns = {img['id']: [] for img in self.images}\n",
    "      for ann in self.annotations:\n",
    "        self.img_to_anns[ann['image_id']].append(ann)\n",
    "    \n",
    "      print(f\"Immagini totali: {len(self.images)}\")\n",
    "      print(f\"Immagini con annotazioni: {len(self.images)}\")\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, str(self.images[idx]['file_name']))\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        img_id = self.images[idx]['id']\n",
    "        annotations = self.img_to_anns[img_id]\n",
    "        \n",
    "        # Conversione annotazioni COCO in Pascal VOC\n",
    "        boxes = []\n",
    "        labels = []  \n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, boxes, labels\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffd431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:30.209434Z",
     "iopub.status.busy": "2025-03-18T16:46:30.209236Z",
     "iopub.status.idle": "2025-03-18T16:46:30.423527Z",
     "shell.execute_reply": "2025-03-18T16:46:30.422624Z"
    },
    "papermill": {
     "duration": 0.217904,
     "end_time": "2025-03-18T16:46:30.424870",
     "exception": false,
     "start_time": "2025-03-18T16:46:30.206966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = GunDataset(train_dataset_folder, train_dataset_folder+\"/_annotations.coco.json\", transform=transform)\n",
    "val_dataset = GunDataset(valid_dataset_folder, valid_dataset_folder+\"/_annotations.coco.json\", transform=transform)\n",
    "\n",
    "num_workers = 2 \n",
    "if os.name == 'nt':\n",
    "    num_workers = 0\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907840bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:30.429692Z",
     "iopub.status.busy": "2025-03-18T16:46:30.429472Z",
     "iopub.status.idle": "2025-03-18T16:46:33.818813Z",
     "shell.execute_reply": "2025-03-18T16:46:33.817928Z"
    },
    "papermill": {
     "duration": 3.393461,
     "end_time": "2025-03-18T16:46:33.820491",
     "exception": false,
     "start_time": "2025-03-18T16:46:30.427030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes=2)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0025, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af501f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:33.827999Z",
     "iopub.status.busy": "2025-03-18T16:46:33.827732Z",
     "iopub.status.idle": "2025-03-18T16:46:37.557997Z",
     "shell.execute_reply": "2025-03-18T16:46:37.557287Z"
    },
    "papermill": {
     "duration": 3.735559,
     "end_time": "2025-03-18T16:46:37.559550",
     "exception": false,
     "start_time": "2025-03-18T16:46:33.823991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import time\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    metric = MeanAveragePrecision(box_format='xyxy', \n",
    "                                 iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (images, targets_boxes, targets_labels) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = []\n",
    "        \n",
    "        for boxes, labels in zip(targets_boxes, targets_labels):\n",
    "            target = {}\n",
    "            target['boxes'] = boxes.to(device)\n",
    "            target['labels'] = labels.to(device)\n",
    "            targets.append(target)\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0)\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0)\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0)\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        model.train()\n",
    "        \n",
    "        metric.update(predictions, targets)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Batch [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\")\n",
    "    \n",
    "    metric_results = metric.compute()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'loss_classifier': loss_classifier / len(data_loader),\n",
    "        'loss_box_reg': loss_box_reg / len(data_loader),\n",
    "        'loss_objectness': loss_objectness / len(data_loader),\n",
    "        'loss_rpn_box_reg': loss_rpn_box_reg / len(data_loader),\n",
    "        'mAP': metric_results['map'].item(),\n",
    "        'mAP_50': metric_results['map_50'].item(),\n",
    "        'mAP_75': metric_results['map_75'].item(),\n",
    "        'recall': metric_results['mar_100'].item(),\n",
    "        'f1': 2 * (metric_results['map'].item() * metric_results['mar_100'].item()) / \n",
    "              (metric_results['map'].item() + metric_results['mar_100'].item() + 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} training results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format='xyxy', \n",
    "                                 iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets_boxes, targets_labels in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = []\n",
    "            \n",
    "            for boxes, labels in zip(targets_boxes, targets_labels):\n",
    "                target = {}\n",
    "                target['boxes'] = boxes.to(device)\n",
    "                target['labels'] = labels.to(device)\n",
    "                targets.append(target)\n",
    "            \n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            loss_classifier += loss_dict.get('loss_classifier', 0)\n",
    "            loss_box_reg += loss_dict.get('loss_box_reg', 0)\n",
    "            loss_objectness += loss_dict.get('loss_objectness', 0)\n",
    "            loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0)\n",
    "            total_loss += losses.item()\n",
    "            \n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "    \n",
    "    metric_results = metric.compute()\n",
    "    \n",
    "    metrics = {\n",
    "        'val_loss': total_loss / len(data_loader),\n",
    "        'val_loss_classifier': loss_classifier / len(data_loader),\n",
    "        'val_loss_box_reg': loss_box_reg / len(data_loader),\n",
    "        'val_loss_objectness': loss_objectness / len(data_loader),\n",
    "        'val_loss_rpn_box_reg': loss_rpn_box_reg / len(data_loader),\n",
    "        'val_mAP': metric_results['map'].item(),\n",
    "        'val_mAP_50': metric_results['map_50'].item(),\n",
    "        'val_mAP_75': metric_results['map_75'].item(),\n",
    "        'val_recall': metric_results['mar_100'].item(),\n",
    "        'val_f1': 2 * (metric_results['map'].item() * metric_results['mar_100'].item()) / \n",
    "                 (metric_results['map'].item() + metric_results['mar_100'].item() + 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nValidation results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, device, num_epochs=10):\n",
    "    best_map = 0\n",
    "    history = []\n",
    "    \n",
    "    #Early stopping\n",
    "    patience = 3\n",
    "    epochs_no_improve = 0\n",
    "    best_model_path = '../Models/best_model_fasterrcnn.pth'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "        scheduler.step()\n",
    "        val_metrics = validate(model, val_loader, device)        \n",
    "        \n",
    "        if val_metrics['val_mAP'] > best_map:\n",
    "            best_map = val_metrics['val_mAP']\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Nessun miglioramento per {epochs_no_improve} epoche. Miglior mAP: {best_map:.4f}\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        epoch_metrics = {**train_metrics, **val_metrics}\n",
    "        history.append(epoch_metrics)\n",
    "        \n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} completata\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95450beb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:37.567039Z",
     "iopub.status.busy": "2025-03-18T16:46:37.566609Z",
     "iopub.status.idle": "2025-03-18T16:46:37.572172Z",
     "shell.execute_reply": "2025-03-18T16:46:37.571342Z"
    },
    "papermill": {
     "duration": 0.010485,
     "end_time": "2025-03-18T16:46:37.573462",
     "exception": false,
     "start_time": "2025-03-18T16:46:37.562977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Errore: il dizionario 'history' è vuoto. Nessun grafico verrà generato.\")\n",
    "        return\n",
    "\n",
    "    metrics_to_plot = [\n",
    "        ('mAP', 'val_mAP', 'Mean Average Precision'),\n",
    "        ('mAP_50', 'val_mAP_50', 'mAP@0.5'),\n",
    "        ('mAP_75', 'val_mAP_75', 'mAP@0.75'),\n",
    "        ('recall', 'val_recall', 'Mean Average Recall'),\n",
    "        ('f1', 'val_f1', 'F1 Score'),\n",
    "        ('loss', 'val_loss', 'Loss'),\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (train_metric, val_metric, title) in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        train_values = df.get(train_metric, pd.Series(dtype=float))\n",
    "        val_values = df.get(val_metric, pd.Series(dtype=float))\n",
    "\n",
    "        if not train_values.empty:\n",
    "            ax.plot(df.index, train_values, 'b-', label='Training', alpha=0.7)\n",
    "        \n",
    "        if not val_values.empty:\n",
    "            ax.plot(df.index, val_values, 'r--', label='Validation', alpha=0.7)\n",
    "            \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoca')\n",
    "        ax.set_ylabel('Valore')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_metrics_{time.strftime(\"%Y%m%d-%H%M%S\")}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a72ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:37.580277Z",
     "iopub.status.busy": "2025-03-18T16:46:37.580065Z",
     "iopub.status.idle": "2025-03-18T21:51:02.702766Z",
     "shell.execute_reply": "2025-03-18T21:51:02.701820Z"
    },
    "papermill": {
     "duration": 18265.127671,
     "end_time": "2025-03-18T21:51:02.704212",
     "exception": false,
     "start_time": "2025-03-18T16:46:37.576541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "history = train_model(model, optimizer, lr_scheduler, train_loader, val_loader, device, num_epochs)\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6894691,
     "sourceId": 11064843,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18285.903386,
   "end_time": "2025-03-18T21:51:05.963398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-18T16:46:20.060012",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
