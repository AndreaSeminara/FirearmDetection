{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14bf7d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:22.570005Z",
     "iopub.status.busy": "2025-03-18T16:46:22.569754Z",
     "iopub.status.idle": "2025-03-18T16:46:30.190219Z",
     "shell.execute_reply": "2025-03-18T16:46:30.189523Z"
    },
    "papermill": {
     "duration": 7.624864,
     "end_time": "2025-03-18T16:46:30.191718",
     "exception": false,
     "start_time": "2025-03-18T16:46:22.566854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.path.abspath(os.sep)\n",
    "train_dataset_folder = \"Dataset/fasterrcnn/train\"\n",
    "valid_dataset_folder = \"Dataset/fasterrcnn/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ffd431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:30.209434Z",
     "iopub.status.busy": "2025-03-18T16:46:30.209236Z",
     "iopub.status.idle": "2025-03-18T16:46:30.423527Z",
     "shell.execute_reply": "2025-03-18T16:46:30.422624Z"
    },
    "papermill": {
     "duration": 0.217904,
     "end_time": "2025-03-18T16:46:30.424870",
     "exception": false,
     "start_time": "2025-03-18T16:46:30.206966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "class GunDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None, train=True):\n",
    "      with open(annotation_dir, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "      self.image_dir = image_dir\n",
    "      self.transform = transform\n",
    "      self.train = train\n",
    "      self.images = coco_data['images']\n",
    "      self.annotations = coco_data['annotations']\n",
    "      self.categories = {}\n",
    "      self.id_to_name = {}\n",
    "      \n",
    "      for category in coco_data['categories']:\n",
    "          new_id = len(self.categories) + 1\n",
    "          self.categories[category['id']] = new_id\n",
    "          self.id_to_name[new_id] = category['name']\n",
    "      \n",
    "      print(\"Categorie trovate:\")\n",
    "      for new_id, name in self.id_to_name.items():\n",
    "          print(f\"- {name}: {new_id}\")\n",
    "      \n",
    "      # Crea il mapping immagine -> annotazioni (solo per armi)\n",
    "      self.img_to_anns = {img['id']: [] for img in self.images}\n",
    "      for ann in self.annotations:\n",
    "          self.img_to_anns[ann['image_id']].append(ann)\n",
    "      \n",
    "      self.images = [img for img in self.images if len(self.img_to_anns[img['id']]) > 0]\n",
    "      \n",
    "      print(f\"Immagini totali con armi: {len(self.images)}\")\n",
    "      print(f\"Annotazioni di armi totali: {len(self.annotations)}\")\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.images[idx]['file_name'])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = np.array(image)\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "\n",
    "        img_id = self.images[idx]['id']\n",
    "        annotations = self.img_to_anns[img_id]\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            \n",
    "            x1 = min(max(x, 0), width)\n",
    "            y1 = min(max(y, 0), height)\n",
    "            x2 = min(max(x + w, 0), width)\n",
    "            y2 = min(max(y + h, 0), height)\n",
    "            \n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(self.categories[ann['category_id']])\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=boxes.numpy().tolist(),\n",
    "                labels=labels.numpy().tolist()\n",
    "            )\n",
    "            image = transformed['image']\n",
    "            \n",
    "            if len(transformed['bboxes']) > 0:\n",
    "                boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
    "                labels = torch.tensor(transformed['labels'], dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros(0, dtype=torch.int64)\n",
    "\n",
    "        return image, {\"boxes\": boxes, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd131db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Rotate(limit=10, p=0.5),\n",
    "    A.MedianBlur(blur_limit=3, p=0.1),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(\n",
    "    format='pascal_voc',\n",
    "    label_fields=['labels'],\n",
    "    min_visibility=0.3,\n",
    "    min_area=0\n",
    "))\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
    "\n",
    "train_dataset = GunDataset(train_dataset_folder, train_dataset_folder+\"/_annotations.coco.json\", transform=train_transform)\n",
    "val_dataset = GunDataset(valid_dataset_folder, valid_dataset_folder+\"/_annotations.coco.json\", transform=val_transform)\n",
    "\n",
    "num_workers = 2 \n",
    "if os.name == 'nt':\n",
    "    num_workers = 0\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907840bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T16:46:30.429692Z",
     "iopub.status.busy": "2025-03-18T16:46:30.429472Z",
     "iopub.status.idle": "2025-03-18T16:46:33.818813Z",
     "shell.execute_reply": "2025-03-18T16:46:33.817928Z"
    },
    "papermill": {
     "duration": 3.393461,
     "end_time": "2025-03-18T16:46:33.820491",
     "exception": false,
     "start_time": "2025-03-18T16:46:30.427030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torch.nn as nn\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "class FasterRCNNPredictorWithDropout(FastRCNNPredictor):\n",
    "  def __init__(self, in_channels, num_classes, dropout_prob=0.3):\n",
    "      super(FasterRCNNPredictorWithDropout, self).__init__(in_channels, num_classes)\n",
    "      self.dropout = nn.Dropout(p=dropout_prob)\n",
    "  \n",
    "  def forward(self, x):\n",
    "      x = self.dropout(x)\n",
    "      scores = self.cls_score(x)\n",
    "      bbox_deltas = self.bbox_pred(x)\n",
    "      return scores, bbox_deltas\n",
    "\n",
    "model.roi_heads.box_predictor = FasterRCNNPredictorWithDropout(in_features, num_classes=len(train_dataset.categories) + 1)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0025, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf01f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "import time\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    metric = MeanAveragePrecision(box_format='xyxy', \n",
    "                                 iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        loss_classifier += loss_dict.get('loss_classifier', 0)\n",
    "        loss_box_reg += loss_dict.get('loss_box_reg', 0)\n",
    "        loss_objectness += loss_dict.get('loss_objectness', 0)\n",
    "        loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(images)\n",
    "        model.train()\n",
    "        \n",
    "        metric.update(predictions, targets)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Batch [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\")\n",
    "    \n",
    "    metric_results = metric.compute()\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'loss_classifier': loss_classifier / len(data_loader),\n",
    "        'loss_box_reg': loss_box_reg / len(data_loader),\n",
    "        'loss_objectness': loss_objectness / len(data_loader),\n",
    "        'loss_rpn_box_reg': loss_rpn_box_reg / len(data_loader),\n",
    "        'mAP': metric_results['map'].item(),\n",
    "        'mAP_50': metric_results['map_50'].item(),\n",
    "        'mAP_75': metric_results['map_75'].item(),\n",
    "        'recall': metric_results['mar_100'].item(),\n",
    "        'f1': 2 * (metric_results['map'].item() * metric_results['mar_100'].item()) / \n",
    "              (metric_results['map'].item() + metric_results['mar_100'].item() + 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} training results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    print(f\"Time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(box_format='xyxy', \n",
    "                                 iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_classifier = 0\n",
    "    loss_box_reg = 0\n",
    "    loss_objectness = 0\n",
    "    loss_rpn_box_reg = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            model.train()\n",
    "            loss_dict = model(images, targets)\n",
    "            model.eval()\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            loss_classifier += loss_dict.get('loss_classifier', 0)\n",
    "            loss_box_reg += loss_dict.get('loss_box_reg', 0)\n",
    "            loss_objectness += loss_dict.get('loss_objectness', 0)\n",
    "            loss_rpn_box_reg += loss_dict.get('loss_rpn_box_reg', 0)\n",
    "            total_loss += losses.item()\n",
    "            \n",
    "            predictions = model(images)\n",
    "            metric.update(predictions, targets)\n",
    "    \n",
    "    metric_results = metric.compute()\n",
    "    \n",
    "    metrics = {\n",
    "        'val_loss': total_loss / len(data_loader),\n",
    "        'val_loss_classifier': loss_classifier / len(data_loader),\n",
    "        'val_loss_box_reg': loss_box_reg / len(data_loader),\n",
    "        'val_loss_objectness': loss_objectness / len(data_loader),\n",
    "        'val_loss_rpn_box_reg': loss_rpn_box_reg / len(data_loader),\n",
    "        'val_mAP': metric_results['map'].item(),\n",
    "        'val_mAP_50': metric_results['map_50'].item(),\n",
    "        'val_mAP_75': metric_results['map_75'].item(),\n",
    "        'val_recall': metric_results['mar_100'].item(),\n",
    "        'val_f1': 2 * (metric_results['map'].item() * metric_results['mar_100'].item()) / \n",
    "                 (metric_results['map'].item() + metric_results['mar_100'].item() + 1e-6)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nValidation results:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_model(model, optimizer, scheduler, train_loader, val_loader, device, num_epochs=10):\n",
    "    best_map = 0\n",
    "    history = []\n",
    "    \n",
    "    #Early stopping\n",
    "    patience = 3\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    best_model_path = 'best_model_fasterrcnn_do_data_augmented.pth'\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "        scheduler.step()\n",
    "        val_metrics = validate(model, val_loader, device)\n",
    "        \n",
    "        if val_metrics['val_mAP'] > best_map:\n",
    "            best_map = val_metrics['val_mAP']\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Nessun miglioramento per {epochs_no_improve} epoche. Miglior mAP: {best_map:.4f}\")\n",
    "            \n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping attivato all'epoca {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        epoch_metrics = {**train_metrics, **val_metrics}\n",
    "        history.append(epoch_metrics)\n",
    "        \n",
    "        print(f\"Epoca {epoch+1}/{num_epochs} completata\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc31506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    df = pd.DataFrame(history)\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Errore: il dizionario 'history' è vuoto. Nessun grafico verrà generato.\")\n",
    "        return\n",
    "\n",
    "    metrics_to_plot = [\n",
    "        ('mAP', 'val_mAP', 'Mean Average Precision'),\n",
    "        ('mAP_50', 'val_mAP_50', 'mAP@0.5'),\n",
    "        ('mAP_75', 'val_mAP_75', 'mAP@0.75'),\n",
    "        ('recall', 'val_recall', 'Mean Average Recall'),\n",
    "        ('f1', 'val_f1', 'F1 Score'),\n",
    "        ('loss', 'val_loss', 'Loss'),\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 20))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, (train_metric, val_metric, title) in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        train_values = df.get(train_metric, pd.Series(dtype=float))\n",
    "        val_values = df.get(val_metric, pd.Series(dtype=float))\n",
    "\n",
    "        if not train_values.empty:\n",
    "            ax.plot(df.index, train_values, 'b-', label='Training', alpha=0.7)\n",
    "        \n",
    "        if not val_values.empty:\n",
    "            ax.plot(df.index, val_values, 'r--', label='Validation', alpha=0.7)\n",
    "            \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoca')\n",
    "        ax.set_ylabel('Valore')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'training_metrics_dropout_data_augmented_{time.strftime(\"%Y%m%d-%H%M%S\")}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "history = train_model(model, optimizer, lr_scheduler, train_loader, val_loader, device, num_epochs)\n",
    "plot_metrics(history)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6894691,
     "sourceId": 11064843,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18285.903386,
   "end_time": "2025-03-18T21:51:05.963398",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-18T16:46:20.060012",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
